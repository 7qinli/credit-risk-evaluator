{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "71235224",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/0g/_n5mqxjj7w73ksp1kj2q0gcw0000gn/T/ipykernel_47905/3463104668.py:100: DeprecationWarning: In a future version, `df.iloc[:, i] = newvals` will attempt to set the values inplace instead of always setting a new array. To retain the old behavior, use either `df[df.columns[i]] = newvals` or, if columns are non-unique, `df.isetitem(i, newvals)`\n",
      "  df_all.iloc[:, -1] = Y\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.pipeline import FeatureUnion\n",
    "from sklearn.impute import MissingIndicator\n",
    "from sklearn.pipeline import Pipeline\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pickle\n",
    "import streamlit as st\n",
    "from scipy.sparse import csr_matrix\n",
    "from sklearn.linear_model import LinearRegression                         \n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn import neighbors                           \n",
    "from sklearn import tree, linear_model                         \n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis \n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn import metrics\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.pipeline import FeatureUnion\n",
    "import joblib\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "import pickle\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_selection import SelectKBest, chi2\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "df = pd.read_csv('heloc_dataset_v1.csv')\n",
    "n_rows = df.shape[0]\n",
    "n_cols = df.shape[1]\n",
    "col_names = df.columns\n",
    "row_indexes = df.index.tolist()\n",
    "df_first3rows = df.head(3)\n",
    "df_last3rows = df.tail(3)\n",
    "df_rows1and3 = df.loc[[1, 3]]\n",
    "df_cols2and4 = df.iloc[:, [2, 4]]\n",
    "series_customer5 = df.loc[5]\n",
    "df_first3rowsAndCols = df.iloc[:3, :3]\n",
    "n_cols_with_missing_values = df.isnull().any(axis=0).sum()\n",
    "n_rows_with_missing_ExternalRiskEstimate_values = df['ExternalRiskEstimate'].isnull().sum()\n",
    "cols_numeric = []\n",
    "cols_string = []\n",
    "for col in col_names:\n",
    "    if df[col].dtype in ['int64', 'float64']:\n",
    "        cols_numeric.append(col)\n",
    "    elif df[col].dtype == 'object':\n",
    "        cols_string.append(col)\n",
    "df_missing_ExternalRiskEstimate = df[df['ExternalRiskEstimate'] == -9]\n",
    "n_rows_with_missing_ExternalRiskEstimate = df_missing_ExternalRiskEstimate.shape[0]\n",
    "df_missing_ExternalRiskEstimate_replaced = df_missing_ExternalRiskEstimate.replace(-9, np.nan)\n",
    "n_rows_all_numeric_missing = df_missing_ExternalRiskEstimate_replaced.select_dtypes(include=np.number).isnull().all(axis=1).sum()\n",
    "df_replaced = df.replace(-9, np.nan)\n",
    "df_without_missing_rows = df[df_replaced[cols_numeric].notna().any(axis=1)]\n",
    "s_minus_7 = (df_without_missing_rows == -7).sum()\n",
    "s_minus_8 = (df_without_missing_rows == -8).sum()\n",
    "s_minus_9 = (df_without_missing_rows == -9).sum()\n",
    "s1 = pd.Series([True,True,False,False])\n",
    "s2 = pd.Series([True,False,True,False])\n",
    "#s1|s2\n",
    "def has_missing_value(row):\n",
    "    return any(row.isin([-7, -8, -9]))\n",
    "s_some_values_are_missing = df_without_missing_rows.apply(has_missing_value, axis=1)\n",
    "grouped = df_without_missing_rows.groupby('RiskPerformance').mean()\n",
    "grouped = grouped.T\n",
    "grouped.columns = ['Bad', 'Good']\n",
    "df_avg_feature_value_per_group = grouped\n",
    "df_without_missing_rows.head()\n",
    "df = df_without_missing_rows\n",
    "first_col = df.pop('RiskPerformance')\n",
    "df.insert(len(df.columns), 'RiskPerformance', first_col)\n",
    "X = df.iloc[:, :23]\n",
    "Y = df.iloc[:, -1]\n",
    "Y = Y.replace({\"Bad\": 1, \"Good\": 0})\n",
    "default_probability = Y.mean()\n",
    "df_all = df\n",
    "df_all.iloc[:, -1] = Y\n",
    "df_train, df_test = train_test_split(df_all, test_size=0.2, random_state=1234)\n",
    "X_train = df_train.iloc[:, :23]\n",
    "X_test  = df_test.iloc[:, :23]\n",
    "Y_train = df_train.iloc[:, -1]\n",
    "Y_test  = df_test.iloc[:, -1]\n",
    "missing_train = X_train['ExternalRiskEstimate'] == -9\n",
    "X_train = X_train.loc[~missing_train, :]\n",
    "Y_train = Y_train.loc[~missing_train]\n",
    "missing_test = X_test['ExternalRiskEstimate'] == -9\n",
    "X_test = X_test.loc[~missing_test, :]\n",
    "Y_test = Y_test.loc[~missing_test]\n",
    "df_count_missing = pd.concat([(X_train==-7).sum(), (X_train==-8).sum(), (X_train==-9).sum()], axis=1)\n",
    "df_count_missing.columns = [-7,-8,-9]\n",
    "do_nothing_imputer = ColumnTransformer([(\"Imputer -7 to mean\", SimpleImputer(missing_values=-7, strategy='mean'), [])], remainder='passthrough')\n",
    "feature_expansion = FeatureUnion([(\"do nothing\", do_nothing_imputer),\n",
    "                                  (\"add features for -7\", MissingIndicator(missing_values=-7, features='missing-only')),\n",
    "                                  (\"add features for -8\", MissingIndicator(missing_values=-8, features='missing-only'))])\n",
    "pipeline = Pipeline([(\"expand features\", feature_expansion), \n",
    "                 (\"replace -7 with -8\", SimpleImputer(missing_values=-7, strategy='constant', fill_value=-8)),\n",
    "                 (\"replace -8 with mean\", SimpleImputer(missing_values=-8, strategy='mean'))])\n",
    "arr_X_train_t = pipeline.fit_transform(X_train)\n",
    "minus_7_indicator_transformer = MissingIndicator(missing_values=-7, features='missing-only').fit(X_train)\n",
    "col_names_minus_7 = X_train.columns.values[minus_7_indicator_transformer.features_].tolist() \n",
    "col_names_minus_7 = list(map(lambda s:str(s)+'=-7',col_names_minus_7)) \n",
    "minus_8_indicator_transformer = MissingIndicator(missing_values=-8, features='missing-only').fit(X_train)\n",
    "col_names_minus_8 = X_train.columns.values[minus_8_indicator_transformer.features_].tolist()\n",
    "col_names_minus_8 = list(map(lambda s:str(s)+'=-8',col_names_minus_8))\n",
    "column_names = X_train.columns.values.tolist() + col_names_minus_7 + col_names_minus_8\n",
    "X_train_t = pd.DataFrame(arr_X_train_t, columns = column_names)\n",
    "new_data =  X_test.iloc[[3,7],:]\n",
    "new_data_t = pipeline.transform(new_data) # Notice that we run transform() and not fit_transform()!\n",
    "new_data_t = pd.DataFrame(new_data_t, columns=column_names)\n",
    "X_test_t = pipeline.transform(X_test) # Notice that we run transform() and not fit_transform()!\n",
    "X_test_t = pd.DataFrame(X_test_t, columns=column_names)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "X_train_t_tr, X_train_t_val, Y_train_t_tr, Y_train_t_val = train_test_split(X_train_t, Y_train, test_size=0.25, random_state=1234)\n",
    "\n",
    "n = len(Y_train_t_tr) + len(Y_train_t_val) + len(Y_test)\n",
    "\n",
    "#log_reg = LogisticRegression(max_iter=10000, random_state=0).fit(X_train_t_tr, Y_train_t_tr) # Logistic regression\n",
    "\n",
    "\n",
    "\n",
    "#param_grid = {'C': np.arange(0.01, 1, 0.01)}\n",
    "#log_reg = LogisticRegression(max_iter=10000, random_state=0)\n",
    "#grid_search = GridSearchCV(log_reg, param_grid, cv=10)\n",
    "#grid_search.fit(X_train_t, Y_train)\n",
    "#log_reg_best = LogisticRegression(max_iter=10000, random_state=0)\n",
    "#log_reg_best.set_params(C=0.09)\n",
    "#for i in range(15,35):\n",
    "selector = SelectKBest(chi2, k=24)\n",
    "X_train_t_selected = selector.fit_transform(X_train_t, Y_train)\n",
    "X_test_t_selected = selector.transform(X_test_t)\n",
    "log_reg_best = LogisticRegression(C=0.09, max_iter=10000, random_state=0).fit(X_train_t_selected, Y_train)\n",
    "test_accuracy = log_reg_best.score(X_test_t_selected, Y_test)\n",
    "\n",
    "\n",
    "\n",
    "log_reg_best.fit(X_train_t_selected, Y_train)\n",
    "\n",
    "\n",
    "\n",
    "pipeline = Pipeline([('selector', selector), ('log_reg_best', log_reg_best)])\n",
    "pipeline.fit(X_train_t_selected, Y_train)\n",
    "y_pred = pipeline.predict(X_test_t_selected)\n",
    "probs = pipeline.predict_proba(X_train_t_selected)\n",
    "test_accuracy = accuracy_score(Y_test, y_pred)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7152fb4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-28 12:25:55.649 \n",
      "  \u001b[33m\u001b[1mWarning:\u001b[0m to view this Streamlit app on a browser, run it with the following\n",
      "  command:\n",
      "\n",
      "    streamlit run /Users/liqin/opt/anaconda3/lib/python3.9/site-packages/ipykernel_launcher.py [ARGUMENTS]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DeltaGenerator(_root_container=0, _provided_cursor=None, _parent=None, _block_type=None, _form_data=None)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('/Users/liqin/Desktop/predp/team/knn_model2.p','rb') as f2:\n",
    "    loaded_model = pickle.load(f2)\n",
    "st.header('Risk Predictor for Home Equity Line of Credit')\n",
    "st.text('This interface has been designed to assist banks in evaluating a client\\'s Home Equity\\nLine of Credit.\\nThe user simply inputs the application profile values for each feature, and based on\\nthe weighting of each feature, a corresponding risk level comment is provided.\\nThis allows the bank to determine the probability of how low/high-risky on the\\napplication and make an informed decision about whether to proceed with the application.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01d4ead8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3835b3b2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dd8525b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac473e70",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ae93313f",
   "metadata": {},
   "outputs": [],
   "source": [
    "x1 = st.slider('Consolidated Version of Risk Markers', 35,100,35,1)\n",
    "\n",
    "\n",
    "x2 = st.slider('Months Since Oldest Trade Open', 5,610,5,1)\n",
    "x3 = st.slider('Months Since Most Recent Trade Open', 0,185,0,1)\n",
    "x4 = st.slider('Average Months in File', 0,260,0,1)\n",
    "x5 = st.slider('Number Satisfactory Trades', 0,80,0,1)\n",
    "x6 = st.slider('Number Trades 60+ Ever', 0,20,0,1)\n",
    "x7 = st.slider('Number Trades 90+ Ever', 0,20,0,1)\n",
    "x8 = st.slider('Percent Trades Never Delinquent', 0,100,0,1)\n",
    "x9 = st.slider('Months Since Most Recent Delinquency', 0,85,0,1)\n",
    "x10 = st.slider('Max Delq/Public Records Last 12 Months', 0,10,0,1)\n",
    "x11 = st.slider('Max Delinquency Ever', 0,10,0,1)\n",
    "x12 = st.slider('Total Number of Credit Accounts', 0,100,0,1)\n",
    "x13 = st.slider('Number of Trades Open in Last 12 Months', 0,20,0,1)\n",
    "x14 = st.slider('Percent Installment Trades', 0,100,0,1)\n",
    "x15 = st.slider('Months Since Most Recent Inquiry Excluding 7days', 0,24,0,1)\n",
    "x16 = st.slider('Number of Inquiry Last 6 Months', 0,30,0,1)\n",
    "x17 = st.slider('Number of Inquiry Last 6 Months Excluding 7days', 0,30,0,1)\n",
    "x18 = st.slider('Net Fraction Revolving Burden', 0,240,0,1)\n",
    "x19 = st.slider('Net Fraction Installment Burden', 0,475,0,1)\n",
    "x20 = st.slider('Number Revolving Trades with Balance', 0,24,0,1)\n",
    "#x21 = st.slider('NumInstallTradesWBalance', 0,20,0,1)\n",
    "\n",
    "x21 = st.slider('Number Bank Trades with High Utilization Ratio', 0,20,0,1)\n",
    "x22 = st.slider('Percent Trades with Balance', 0,100,0,1)\n",
    "x23 = st.slider('If There Is No Months Since Most Recent Delinquency', 0,1,0,1) #MSinceMostRecentDelq=-7 9\n",
    "\n",
    "\n",
    "#x25 = st.slider('MSinceMostRecentInqexcl7days=-7', 0,1,0,1)\n",
    "#x26 = st.slider('MSinceOldestTradeOpen=-8', 5.0,605.0,5.0,0.1)\n",
    "#x27 = st.slider('MSinceMostRecentDelq=-8', 5.0,605.0,5.0,0.1)\n",
    "x24 = st.slider('If There Are Non-usable/unvalid Trades or Inquiries', 0,1,0,1) #MSinceMostRecentInqexcl7days=-8 15\n",
    "#x27 = st.slider('NetFractionRevolvingBurden=-8', 0,1,0,1)\n",
    "#x28 = st.slider('NetFractionInstallBurden=-8', 0,1,0,1)\n",
    "#x29 = st.slider('NumRevolvingTradesWBalance=-8', 0,1,0,1)\n",
    "#x32 = st.slider('NumInstallTradesWBalance=-8', 5.0,605.0,5.0,0.1)\n",
    "#x30 = st.slider('NumBank2NatlTradesWHighUtilization=-8', 0,1,0,1)\n",
    "#x34 = st.slider('PercentTradesWBalance=-8', 5.0,605.0,5.0,0.1)\n",
    "\n",
    "\n",
    "\n",
    "input_value = [x1,x2,x3,x4,x5,x6,x7,x8,x9,x10,x11,x12,x13,x14,x15,x16,x17,x18,x19,x20,x21,x22,x23,x24]\n",
    "#X_values = X_train_t_selected\n",
    "#print(loaded_model.predict(X_values[:3,:])) # make predictions for the first 3 observations\n",
    "prediction = loaded_model.predict_proba([input_value])[0]\n",
    "\n",
    "\n",
    "if prediction[0] > prediction[1]:\n",
    "    st.text('This application has a %.2f%% probability to be low risk.' % (prediction[0]*100))\n",
    "else:\n",
    "    st.text('This application has a %.2f%% probability to be high risk.' % (prediction[1]*100))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6b2dd21",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
